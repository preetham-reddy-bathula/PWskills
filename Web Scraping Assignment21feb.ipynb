{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites using automated software. The software (also known as a web scraper) navigates through the website and collects the relevant data in a structured format, such as a spreadsheet or database. Web scraping is used to gather data from websites that don't offer an API or data feed, or when the available data is not sufficient or not in a format that can be easily analyzed.\n",
    "\n",
    "Three areas where web scraping is commonly used are:\n",
    "\n",
    "Business intelligence: Web scraping can be used to gather data on competitors, market trends, and consumer behavior. This data can be analyzed to make informed business decisions.\n",
    "\n",
    "Academic research: Web scraping can be used to collect data for academic research, such as analyzing trends in online behavior, tracking the spread of misinformation, and gathering data on social networks.\n",
    "\n",
    "E-commerce: Web scraping can be used to gather data on product prices, customer reviews, and inventory levels from e-commerce websites. This data can be used to optimize pricing strategies, improve product descriptions, and monitor competitor activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods for web scraping, including:\n",
    "\n",
    "Parsing HTML: This involves using programming languages like Python or PHP to parse the HTML of a website and extract the relevant data.\n",
    "\n",
    "Using APIs: Some websites offer APIs that can be used to retrieve data in a structured format.\n",
    "\n",
    "Automated tools: There are several automated web scraping tools available, such as Scrapy, Beautiful Soup, and Selenium.\n",
    "\n",
    "Browser extensions: Some browser extensions, such as Web Scraper and Data Miner, allow users to extract data from websites without writing code.\n",
    "\n",
    "Manual copying and pasting: This involves manually copying and pasting data from websites into a spreadsheet or database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for web scraping purposes to extract the data from HTML and XML files. It provides a set of tools for parsing HTML and XML documents, allowing developers to extract data from web pages and manipulate it in a structured format. Beautiful Soup is used because it is easy to use, efficient, and flexible. It can parse even poorly formed HTML and can handle changes to the website structure gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask is a Python web framework used for building web applications. In this web scraping project, Flask is used to create a web application that allows users to input search terms and retrieve data from a website. Flask provides a simple, lightweight way to create web applications, making it ideal for this type of project. Flask also provides a lot of flexibility, allowing developers to customize the application to suit their specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic bean and AWS codePipeLine are used in this project.\n",
    "Elastic Beanstalk is a fully-managed service offered by AWS that simplifies the process of deploying and managing web applications. It automates the deployment process by automatically handling the capacity provisioning, load balancing, scaling, and application health monitoring, which allows developers to focus on writing the code for their application.\n",
    "\n",
    "Elastic Beanstalk supports several programming languages, such as Java, .NET, PHP, Node.js, Python, Ruby, and Go. It provides a web-based console and a command-line interface to deploy, manage, and monitor web applications. Elastic Beanstalk also integrates with other AWS services, such as Amazon RDS, Amazon S3, and Amazon CloudFront.\n",
    "\n",
    "AWS CodePipeline, on the other hand, is a fully-managed continuous delivery service that helps developers automate the release process for their applications. It provides a workflow that automates the build, test, and deployment phases of a software release process. It also allows developers to build, test, and deploy their applications in a safe and reliable manner, with the ability to roll back to a previous version if necessary.\n",
    "\n",
    "CodePipeline supports integrations with several development tools, such as AWS CodeBuild, AWS CodeDeploy, AWS CloudFormation, and third-party tools such as Jenkins and GitHub. CodePipeline also integrates with Elastic Beanstalk to provide an end-to-end continuous delivery pipeline for web applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
